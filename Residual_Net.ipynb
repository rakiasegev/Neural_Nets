{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        A residual block consists of two dense layers,\n",
    "        where the input is added back before the second\n",
    "        activation function is applied.\n",
    "        If the input is x, then the block computes\n",
    "        relu(W_2 * relu(W_1 * x + b_1) + b_2 + x).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, input_size) # defines first layer\n",
    "        self.layer2 = nn.Linear(input_size, input_size) # defines second layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.relu(self.layer1(x)) # computes relu(W_1 * x + b_1)\n",
    "        output2 = self.relu(self.layer2(output1)+x) # computes relu(W_2 * relu(W_1 * x + b_1) + b_2 + x).\n",
    "        return output2\n",
    "    \n",
    "class ResidualNet(nn.Module):\n",
    "    \"\"\"\n",
    "        A residual net is a neural net where the values at prior layers\n",
    "        are added to the value computed in later layers. Thus the layers\n",
    "        learn what change is needed to the data which can help the model\n",
    "        converge. Features is the dimension of the input, number of blocks\n",
    "        is how many residual blocks will appear in the neural net,\n",
    "        while classes is the number of classification categories.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, number_of_blocks, hidden_size, classes):\n",
    "        super(ResidualNet, self).__init__()\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        self.inputgate = nn.Linear(features, hidden_size) # create input gate for res net\n",
    "        self.outputgate = nn.Linear(hidden_size, classes)\n",
    "\n",
    "        for _ in range(number_of_blocks):\n",
    "            self.resblocks.append(ResidualBlock(hidden_size))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        output1 = self.inputgate(x)\n",
    "        \n",
    "        for block in self.resblocks:\n",
    "            output1 = block(output1)\n",
    "            \n",
    "        return F.log_softmax(self.outputgate(output1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mnist_model(model, num_epochs=20):\n",
    "    training_data = pd.read_csv(\"mnist_train.csv\", header = None).values\n",
    "    training_labels = torch.LongTensor(training_data[:, 0])\n",
    "    training_values = torch.FloatTensor(training_data[:, 1:].astype(float))\n",
    "    \n",
    "    training_dataset = data.TensorDataset(training_values, training_labels)    \n",
    "    loader_dset_train = data.DataLoader(training_dataset, batch_size=128, \n",
    "                                        num_workers=4, shuffle=True)\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_loss = float('inf')\n",
    "    model.train(True)\n",
    "\n",
    "    curr_loss = 0.0\n",
    "    total_batch_number = 0\n",
    "    all_losses = []\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.NLLLoss()\n",
    "    lr_scheduler = exp_lr_scheduler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        optimizer = lr_scheduler(optimizer, epoch)\n",
    "\n",
    "        epoch_running_loss = 0.0\n",
    "        current_batch = 0\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in loader_dset_train:\n",
    "            current_batch += 1\n",
    "            total_batch_number += 1\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), \\\n",
    "                             Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            epoch_running_loss += loss.data[0]\n",
    "            curr_loss += loss.data[0]\n",
    "\n",
    "            if total_batch_number % 100 == 0:\n",
    "                all_losses.append(curr_loss / 100)\n",
    "                time_elapsed = time.time() - since\n",
    "\n",
    "                print('Epoch Number: {}, Batch Number: {}, Loss: {:.4f}'.format(\n",
    "                    epoch, current_batch, curr_loss))\n",
    "                print('Time so far is {:.0f}m {:.0f}s'.format(\n",
    "                    time_elapsed // 60, time_elapsed % 60))\n",
    "                curr_loss = 0.0\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_running_loss < best_loss:\n",
    "            best_loss = epoch_running_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    model.train(False)\n",
    "\n",
    "    return best_model, all_losses\n",
    "    \n",
    "    \n",
    "def accuracy_for_a_category(model, dataset, category):\n",
    "    positive_results = 0\n",
    "    total = 0\n",
    "    for datapoint, label in dataset: # creates loop to compute accuracy for all datapoints in dataset\n",
    "        if label == category:\n",
    "            total += 1 \n",
    "            _, guess = torch.max(model(Variable(datapoint)), 0)\n",
    "            positive_results += torch.sum(guess.data == torch.LongTensor([label]))\n",
    "    return positive_results/total\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "    Given a list of losses, this creates a plot of the loss curve.\n",
    "\"\"\"\n",
    "def plot_loss_curve(losses):\n",
    "    plt.plot(losses) # creates the matplot for losses over time\n",
    "\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "    \n",
    "\"\"\"\n",
    "    Given a model that classifies data points and a labeled data set this\n",
    "    determines the overall accuracy of the model.\n",
    "\"\"\"\n",
    "def test_model(model, dataset):\n",
    "    positive_results = 0\n",
    "    total = len(dataset)\n",
    "    for datapoint, label in dataset: \n",
    "        _, guess = torch.max(model(Variable(datapoint)), 0)\n",
    "        positive_results += torch.sum(guess.data == torch.LongTensor([label]))   \n",
    "    return positive_results/total    \n",
    "    \n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_loss = float('inf')\n",
    "    model.train(True)\n",
    "\n",
    "    curr_loss = 0.0\n",
    "    total_batch_number = 0\n",
    "    all_losses = []\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.NLLLoss()\n",
    "    lr_scheduler = exp_lr_scheduler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        optimizer = lr_scheduler(optimizer, epoch)\n",
    "\n",
    "        epoch_running_loss = 0.0\n",
    "        current_batch = 0\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in loader_dset_train:\n",
    "            current_batch += 1\n",
    "            total_batch_number += 1\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), \\\n",
    "                             Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            epoch_running_loss += loss.data[0]\n",
    "            curr_loss += loss.data[0]\n",
    "\n",
    "            if total_batch_number % 100 == 0:\n",
    "                all_losses.append(curr_loss / 100)\n",
    "                time_elapsed = time.time() - since\n",
    "\n",
    "                print('Epoch Number: {}, Batch Number: {}, Loss: {:.4f}'.format(\n",
    "                    epoch, current_batch, curr_loss))\n",
    "                print('Time so far is {:.0f}m {:.0f}s'.format(\n",
    "                    time_elapsed // 60, time_elapsed % 60))\n",
    "                curr_loss = 0.0\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_running_loss < best_loss:\n",
    "            best_loss = epoch_running_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    model.train(False)\n",
    "\n",
    "    return best_model, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "LR is set to 0.001\n",
      "Epoch Number: 0, Batch Number: 100, Loss: 380.5866\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 0, Batch Number: 200, Loss: 79.1261\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 0, Batch Number: 300, Loss: 43.8147\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 0, Batch Number: 400, Loss: 37.1338\n",
      "Time so far is 0m 3s\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "Epoch Number: 1, Batch Number: 31, Loss: 33.1883\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 1, Batch Number: 131, Loss: 28.9238\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 1, Batch Number: 231, Loss: 29.8364\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 1, Batch Number: 331, Loss: 25.8494\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 1, Batch Number: 431, Loss: 27.1824\n",
      "Time so far is 0m 6s\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "Epoch Number: 2, Batch Number: 62, Loss: 24.7139\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 2, Batch Number: 162, Loss: 24.1034\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 2, Batch Number: 262, Loss: 23.6150\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 2, Batch Number: 362, Loss: 22.7547\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 2, Batch Number: 462, Loss: 22.3443\n",
      "Time so far is 0m 10s\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "Epoch Number: 3, Batch Number: 93, Loss: 19.9035\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 3, Batch Number: 193, Loss: 20.6105\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 3, Batch Number: 293, Loss: 19.3308\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 3, Batch Number: 393, Loss: 19.2531\n",
      "Time so far is 0m 14s\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "Epoch Number: 4, Batch Number: 24, Loss: 20.0570\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 4, Batch Number: 124, Loss: 16.5420\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 4, Batch Number: 224, Loss: 17.9715\n",
      "Time so far is 0m 16s\n",
      "Epoch Number: 4, Batch Number: 324, Loss: 18.8288\n",
      "Time so far is 0m 17s\n",
      "Epoch Number: 4, Batch Number: 424, Loss: 17.9556\n",
      "Time so far is 0m 18s\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "Epoch Number: 5, Batch Number: 55, Loss: 15.1728\n",
      "Time so far is 0m 19s\n",
      "Epoch Number: 5, Batch Number: 155, Loss: 17.1117\n",
      "Time so far is 0m 19s\n",
      "Epoch Number: 5, Batch Number: 255, Loss: 18.0162\n",
      "Time so far is 0m 20s\n",
      "Epoch Number: 5, Batch Number: 355, Loss: 15.5315\n",
      "Time so far is 0m 21s\n",
      "Epoch Number: 5, Batch Number: 455, Loss: 16.1118\n",
      "Time so far is 0m 22s\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "Epoch Number: 6, Batch Number: 86, Loss: 14.4431\n",
      "Time so far is 0m 23s\n",
      "Epoch Number: 6, Batch Number: 186, Loss: 15.0761\n",
      "Time so far is 0m 24s\n",
      "Epoch Number: 6, Batch Number: 286, Loss: 15.0395\n",
      "Time so far is 0m 24s\n",
      "Epoch Number: 6, Batch Number: 386, Loss: 15.4259\n",
      "Time so far is 0m 25s\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "LR is set to 0.0001\n",
      "Epoch Number: 7, Batch Number: 17, Loss: 16.1441\n",
      "Time so far is 0m 26s\n",
      "Epoch Number: 7, Batch Number: 117, Loss: 10.3021\n",
      "Time so far is 0m 27s\n",
      "Epoch Number: 7, Batch Number: 217, Loss: 10.1619\n",
      "Time so far is 0m 28s\n",
      "Epoch Number: 7, Batch Number: 317, Loss: 9.0940\n",
      "Time so far is 0m 28s\n",
      "Epoch Number: 7, Batch Number: 417, Loss: 9.6725\n",
      "Time so far is 0m 29s\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "Epoch Number: 8, Batch Number: 48, Loss: 8.9600\n",
      "Time so far is 0m 30s\n",
      "Epoch Number: 8, Batch Number: 148, Loss: 7.8711\n",
      "Time so far is 0m 31s\n",
      "Epoch Number: 8, Batch Number: 248, Loss: 9.1648\n",
      "Time so far is 0m 32s\n",
      "Epoch Number: 8, Batch Number: 348, Loss: 8.7791\n",
      "Time so far is 0m 33s\n",
      "Epoch Number: 8, Batch Number: 448, Loss: 8.8972\n",
      "Time so far is 0m 34s\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "Epoch Number: 9, Batch Number: 79, Loss: 8.3213\n",
      "Time so far is 0m 34s\n",
      "Epoch Number: 9, Batch Number: 179, Loss: 8.2777\n",
      "Time so far is 0m 35s\n",
      "Epoch Number: 9, Batch Number: 279, Loss: 9.2997\n",
      "Time so far is 0m 36s\n",
      "Epoch Number: 9, Batch Number: 379, Loss: 7.9732\n",
      "Time so far is 0m 37s\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "Epoch Number: 10, Batch Number: 10, Loss: 7.6889\n",
      "Time so far is 0m 38s\n",
      "Epoch Number: 10, Batch Number: 110, Loss: 7.7255\n",
      "Time so far is 0m 39s\n",
      "Epoch Number: 10, Batch Number: 210, Loss: 8.3589\n",
      "Time so far is 0m 40s\n",
      "Epoch Number: 10, Batch Number: 310, Loss: 8.1474\n",
      "Time so far is 0m 40s\n",
      "Epoch Number: 10, Batch Number: 410, Loss: 7.8268\n",
      "Time so far is 0m 41s\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "Epoch Number: 11, Batch Number: 41, Loss: 8.2364\n",
      "Time so far is 0m 42s\n",
      "Epoch Number: 11, Batch Number: 141, Loss: 7.1303\n",
      "Time so far is 0m 43s\n",
      "Epoch Number: 11, Batch Number: 241, Loss: 8.5504\n",
      "Time so far is 0m 44s\n",
      "Epoch Number: 11, Batch Number: 341, Loss: 7.9209\n",
      "Time so far is 0m 44s\n",
      "Epoch Number: 11, Batch Number: 441, Loss: 7.7064\n",
      "Time so far is 0m 45s\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "Epoch Number: 12, Batch Number: 72, Loss: 7.5649\n",
      "Time so far is 0m 46s\n",
      "Epoch Number: 12, Batch Number: 172, Loss: 7.5961\n",
      "Time so far is 0m 47s\n",
      "Epoch Number: 12, Batch Number: 272, Loss: 8.2231\n",
      "Time so far is 0m 48s\n",
      "Epoch Number: 12, Batch Number: 372, Loss: 8.0301\n",
      "Time so far is 0m 49s\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "Epoch Number: 13, Batch Number: 3, Loss: 7.3407\n",
      "Time so far is 0m 49s\n",
      "Epoch Number: 13, Batch Number: 103, Loss: 7.1320\n",
      "Time so far is 0m 50s\n",
      "Epoch Number: 13, Batch Number: 203, Loss: 7.8207\n",
      "Time so far is 0m 51s\n",
      "Epoch Number: 13, Batch Number: 303, Loss: 8.0961\n",
      "Time so far is 0m 52s\n",
      "Epoch Number: 13, Batch Number: 403, Loss: 7.6041\n",
      "Time so far is 0m 53s\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "Epoch Number: 14, Batch Number: 34, Loss: 6.7814\n",
      "Time so far is 0m 53s\n",
      "Epoch Number: 14, Batch Number: 134, Loss: 7.4823\n",
      "Time so far is 0m 54s\n",
      "Epoch Number: 14, Batch Number: 234, Loss: 6.9893\n",
      "Time so far is 0m 55s\n",
      "Epoch Number: 14, Batch Number: 334, Loss: 7.3681\n",
      "Time so far is 0m 56s\n",
      "Epoch Number: 14, Batch Number: 434, Loss: 6.8049\n",
      "Time so far is 0m 57s\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "Epoch Number: 15, Batch Number: 65, Loss: 7.3540\n",
      "Time so far is 0m 58s\n",
      "Epoch Number: 15, Batch Number: 165, Loss: 6.8550\n",
      "Time so far is 0m 59s\n",
      "Epoch Number: 15, Batch Number: 265, Loss: 7.2602\n",
      "Time so far is 0m 59s\n",
      "Epoch Number: 15, Batch Number: 365, Loss: 6.9055\n",
      "Time so far is 1m 0s\n",
      "Epoch Number: 15, Batch Number: 465, Loss: 7.2934\n",
      "Time so far is 1m 1s\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "Epoch Number: 16, Batch Number: 96, Loss: 6.9944\n",
      "Time so far is 1m 2s\n",
      "Epoch Number: 16, Batch Number: 196, Loss: 6.7847\n",
      "Time so far is 1m 3s\n",
      "Epoch Number: 16, Batch Number: 296, Loss: 6.8323\n",
      "Time so far is 1m 3s\n",
      "Epoch Number: 16, Batch Number: 396, Loss: 7.8168\n",
      "Time so far is 1m 4s\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "Epoch Number: 17, Batch Number: 27, Loss: 7.0544\n",
      "Time so far is 1m 5s\n",
      "Epoch Number: 17, Batch Number: 127, Loss: 6.8551\n",
      "Time so far is 1m 6s\n",
      "Epoch Number: 17, Batch Number: 227, Loss: 7.0422\n",
      "Time so far is 1m 7s\n",
      "Epoch Number: 17, Batch Number: 327, Loss: 6.9620\n",
      "Time so far is 1m 8s\n",
      "Epoch Number: 17, Batch Number: 427, Loss: 7.0012\n",
      "Time so far is 1m 8s\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "Epoch Number: 18, Batch Number: 58, Loss: 7.4527\n",
      "Time so far is 1m 9s\n",
      "Epoch Number: 18, Batch Number: 158, Loss: 7.0649\n",
      "Time so far is 1m 10s\n",
      "Epoch Number: 18, Batch Number: 258, Loss: 7.2376\n",
      "Time so far is 1m 11s\n",
      "Epoch Number: 18, Batch Number: 358, Loss: 6.9817\n",
      "Time so far is 1m 12s\n",
      "Epoch Number: 18, Batch Number: 458, Loss: 6.5454\n",
      "Time so far is 1m 13s\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "Epoch Number: 19, Batch Number: 89, Loss: 6.8606\n",
      "Time so far is 1m 14s\n",
      "Epoch Number: 19, Batch Number: 189, Loss: 6.7996\n",
      "Time so far is 1m 15s\n",
      "Epoch Number: 19, Batch Number: 289, Loss: 7.2978\n",
      "Time so far is 1m 16s\n",
      "Epoch Number: 19, Batch Number: 389, Loss: 6.4984\n",
      "Time so far is 1m 16s\n",
      "\n",
      "Training complete in 1m 17s\n",
      "Best loss: 32.934846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ResidualNet (\n",
       "   (resblocks): ModuleList (\n",
       "     (0): ResidualBlock (\n",
       "       (relu): ReLU ()\n",
       "       (layer1): Linear (200 -> 200)\n",
       "       (layer2): Linear (200 -> 200)\n",
       "     )\n",
       "     (1): ResidualBlock (\n",
       "       (relu): ReLU ()\n",
       "       (layer1): Linear (200 -> 200)\n",
       "       (layer2): Linear (200 -> 200)\n",
       "     )\n",
       "   )\n",
       "   (inputgate): Linear (784 -> 200)\n",
       "   (outputgate): Linear (200 -> 10)\n",
       " ),\n",
       " [3.8058663994073867,\n",
       "  0.7912612327933312,\n",
       "  0.4381474824249744,\n",
       "  0.3713379468023777,\n",
       "  0.33188278675079347,\n",
       "  0.28923782631754874,\n",
       "  0.2983643037080765,\n",
       "  0.25849361181259156,\n",
       "  0.2718240737915039,\n",
       "  0.24713867701590062,\n",
       "  0.2410336735472083,\n",
       "  0.23614953093230726,\n",
       "  0.22754704304039478,\n",
       "  0.2234434736520052,\n",
       "  0.19903506878763438,\n",
       "  0.2061052107065916,\n",
       "  0.19330846335738897,\n",
       "  0.19253072738647461,\n",
       "  0.20056950073689223,\n",
       "  0.16542035318911075,\n",
       "  0.17971450101584197,\n",
       "  0.18828803643584252,\n",
       "  0.17955579835921526,\n",
       "  0.15172808557748796,\n",
       "  0.1711172527819872,\n",
       "  0.18016193397343158,\n",
       "  0.15531510528177023,\n",
       "  0.16111846249550582,\n",
       "  0.14443146768957377,\n",
       "  0.15076079146936536,\n",
       "  0.15039538953453302,\n",
       "  0.15425913069397212,\n",
       "  0.16144096199423075,\n",
       "  0.10302084783092141,\n",
       "  0.10161905343644322,\n",
       "  0.09094017994590103,\n",
       "  0.09672530453652144,\n",
       "  0.0895996711589396,\n",
       "  0.0787109531275928,\n",
       "  0.09164838434197009,\n",
       "  0.08779121965169906,\n",
       "  0.0889715152606368,\n",
       "  0.08321256585419178,\n",
       "  0.08277680970728397,\n",
       "  0.09299705628305674,\n",
       "  0.07973189400509,\n",
       "  0.07688895493745804,\n",
       "  0.07725506853312254,\n",
       "  0.08358934374060482,\n",
       "  0.08147410201840102,\n",
       "  0.07826780095696449,\n",
       "  0.08236413626931607,\n",
       "  0.07130276557058096,\n",
       "  0.08550379834137857,\n",
       "  0.0792091422341764,\n",
       "  0.07706432022619993,\n",
       "  0.07564865186810493,\n",
       "  0.0759614789672196,\n",
       "  0.08223105866461992,\n",
       "  0.08030140524730087,\n",
       "  0.07340715140104294,\n",
       "  0.07131952307187021,\n",
       "  0.0782068139500916,\n",
       "  0.08096100169233977,\n",
       "  0.07604123406112194,\n",
       "  0.06781368465162814,\n",
       "  0.07482253035530448,\n",
       "  0.06989306909032166,\n",
       "  0.07368131346069276,\n",
       "  0.06804873103275895,\n",
       "  0.07353994517587126,\n",
       "  0.06854990121908486,\n",
       "  0.07260173140093684,\n",
       "  0.06905505996663124,\n",
       "  0.07293423413299024,\n",
       "  0.0699436616525054,\n",
       "  0.0678466553427279,\n",
       "  0.068322689156048,\n",
       "  0.07816842500120401,\n",
       "  0.07054373010993004,\n",
       "  0.06855068273842335,\n",
       "  0.07042213149368763,\n",
       "  0.06961953058838845,\n",
       "  0.07001226513646543,\n",
       "  0.07452685880474746,\n",
       "  0.07064898212440313,\n",
       "  0.07237580098211766,\n",
       "  0.06981704234611243,\n",
       "  0.0654535178374499,\n",
       "  0.06860620468854904,\n",
       "  0.06799581871833653,\n",
       "  0.07297798769548536,\n",
       "  0.06498444067779928])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should work once you finish coding the residual net. After training it,\n",
    "# you can optionally use some of your code from part c of the prior problem\n",
    "# to see how well the model did.\n",
    "mnist_model = ResidualNet(784, 2, 200, 10)\n",
    "train_mnist_model(mnist_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"mnist_test.csv\", header = None).values\n",
    "test_labels, test_values = torch.LongTensor(test_data[:, 0]), torch.FloatTensor(test_data[:, 1:].astype(float))\n",
    "\n",
    "test_dataset = data.TensorDataset(test_values, test_labels)    \n",
    "loader_dset_test = data.DataLoader(test_dataset, batch_size=128, \n",
    "                                   num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the digit 0 is 0.99\n",
      "The accuracy on the digit 1 is 0.99\n",
      "The accuracy on the digit 2 is 0.95\n",
      "The accuracy on the digit 3 is 0.96\n",
      "The accuracy on the digit 4 is 0.96\n",
      "The accuracy on the digit 5 is 0.95\n",
      "The accuracy on the digit 6 is 0.97\n",
      "The accuracy on the digit 7 is 0.96\n",
      "The accuracy on the digit 8 is 0.96\n",
      "The accuracy on the digit 9 is 0.94\n",
      "The overall accuracy of the 2 layer neural net on MNIST is 0.96\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    accuracy_i = accuracy_for_a_category(mnist_model, test_dataset, i)\n",
    "    print(\"The accuracy on the digit {} is {:.2f}\".format(i, accuracy_i))\n",
    "\n",
    "# plot_loss_curve(losses)\n",
    "# plt.show()\n",
    "\n",
    "accuracy = test_model(mnist_model, test_dataset)\n",
    "print(\"The overall accuracy of the 2 block neural net on MNIST is {:.2f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
