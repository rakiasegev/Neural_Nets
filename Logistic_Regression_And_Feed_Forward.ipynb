{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"\n",
    "        Creates a logistic regression model. Features refers to the dimension\n",
    "        of the inputs, while categories is the number of possible classifications.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, categories):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        # construct and store matrix categories x features\n",
    "        self.weights = nn.Parameter(torch.zeros(categories, features)) \n",
    "        self.biases = nn.Parameter(torch.zeros(categories))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # takes weights and biases and returns probability vectors\n",
    "        return F.log_softmax(torch.addmm(self.biases, x, self.weights.transpose(0, 1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Creates a neural net with two hidden layers and a softmax layer. \n",
    "        Features refers to the dimension of the inputs, hidden1 and hidden2\n",
    "        refer to the number of neurons in the two hidden layers (the dimension of those\n",
    "        layers), and categories refers to the number of possible classifications.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, hidden1, hidden2, categories):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid() # define sigmoid function\n",
    "        self.layer1 = nn.Linear(features, hidden1) # defines first hidden layer \n",
    "        self.layer2 = nn.Linear(hidden1, hidden2) # defines second hidden layer\n",
    "        self.softmaxlayer = nn.Linear(hidden2, categories) # defines softmax layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output1 = self.sigmoid(self.layer1(x)) # computes first layer\n",
    "        output2 = self.sigmoid(self.layer2(output1)) # computes second layer\n",
    "        softmax = F.log_softmax(self.softmaxlayer(output2)) # computes log softmax layer\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "# This is the function to call to train model. It returns the trained model\n",
    "# and a list of losses.\n",
    "\n",
    "def train_mnist_model(model, num_epochs=20):\n",
    "    training_data = pd.read_csv(\"mnist_train.csv\", header = None).values\n",
    "    training_labels = torch.LongTensor(training_data[:, 0])\n",
    "    training_values = torch.FloatTensor(training_data[:, 1:].astype(float))\n",
    "    \n",
    "    training_dataset = data.TensorDataset(training_values, training_labels)    \n",
    "    loader_dset_train = data.DataLoader(training_dataset, batch_size=128, \n",
    "                                        num_workers=4, shuffle=True)\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_loss = float('inf')\n",
    "    model.train(True)\n",
    "\n",
    "    curr_loss = 0.0\n",
    "    total_batch_number = 0\n",
    "    all_losses = []\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.NLLLoss()\n",
    "    lr_scheduler = exp_lr_scheduler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        optimizer = lr_scheduler(optimizer, epoch)\n",
    "\n",
    "        epoch_running_loss = 0.0\n",
    "        current_batch = 0\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in loader_dset_train:\n",
    "            current_batch += 1\n",
    "            total_batch_number += 1\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), \\\n",
    "                             Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            epoch_running_loss += loss.data[0]\n",
    "            curr_loss += loss.data[0]\n",
    "\n",
    "            if total_batch_number % 100 == 0:\n",
    "                all_losses.append(curr_loss / 100)\n",
    "                time_elapsed = time.time() - since\n",
    "\n",
    "                print('Epoch Number: {}, Batch Number: {}, Loss: {:.4f}'.format(\n",
    "                    epoch, current_batch, curr_loss))\n",
    "                print('Time so far is {:.0f}m {:.0f}s'.format(\n",
    "                    time_elapsed // 60, time_elapsed % 60))\n",
    "                curr_loss = 0.0\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_running_loss < best_loss:\n",
    "            best_loss = epoch_running_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    model.train(False)\n",
    "\n",
    "    return best_model, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "LR is set to 0.001\n",
      "Epoch Number: 0, Batch Number: 100, Loss: 14432.0940\n",
      "Time so far is 0m 0s\n",
      "Epoch Number: 0, Batch Number: 200, Loss: 6785.6218\n",
      "Time so far is 0m 0s\n",
      "Epoch Number: 0, Batch Number: 300, Loss: 5284.3545\n",
      "Time so far is 0m 0s\n",
      "Epoch Number: 0, Batch Number: 400, Loss: 5703.9029\n",
      "Time so far is 0m 1s\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "Epoch Number: 1, Batch Number: 31, Loss: 5576.3550\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 1, Batch Number: 131, Loss: 5048.6640\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 1, Batch Number: 231, Loss: 5182.0155\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 1, Batch Number: 331, Loss: 5462.7472\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 1, Batch Number: 431, Loss: 5420.0748\n",
      "Time so far is 0m 1s\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "Epoch Number: 2, Batch Number: 62, Loss: 5632.7343\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 2, Batch Number: 162, Loss: 4823.0620\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 2, Batch Number: 262, Loss: 5464.2665\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 2, Batch Number: 362, Loss: 4531.9432\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 2, Batch Number: 462, Loss: 4822.2329\n",
      "Time so far is 0m 2s\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "Epoch Number: 3, Batch Number: 93, Loss: 5386.9011\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 3, Batch Number: 193, Loss: 5341.1002\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 3, Batch Number: 293, Loss: 5714.9946\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 3, Batch Number: 393, Loss: 5192.1249\n",
      "Time so far is 0m 3s\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "Epoch Number: 4, Batch Number: 24, Loss: 5192.2398\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 4, Batch Number: 124, Loss: 4829.1982\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 4, Batch Number: 224, Loss: 4807.0770\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 4, Batch Number: 324, Loss: 5033.6943\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 4, Batch Number: 424, Loss: 5762.9163\n",
      "Time so far is 0m 3s\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "Epoch Number: 5, Batch Number: 55, Loss: 4382.4532\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 5, Batch Number: 155, Loss: 4741.2166\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 5, Batch Number: 255, Loss: 4878.8043\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 5, Batch Number: 355, Loss: 5008.6161\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 5, Batch Number: 455, Loss: 5080.7003\n",
      "Time so far is 0m 4s\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "Epoch Number: 6, Batch Number: 86, Loss: 4960.7510\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 6, Batch Number: 186, Loss: 4562.8366\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 6, Batch Number: 286, Loss: 5237.2683\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 6, Batch Number: 386, Loss: 4763.8809\n",
      "Time so far is 0m 5s\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "LR is set to 0.0001\n",
      "Epoch Number: 7, Batch Number: 17, Loss: 5571.7891\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 7, Batch Number: 117, Loss: 3112.7091\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 7, Batch Number: 217, Loss: 3027.6601\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 7, Batch Number: 317, Loss: 2631.0790\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 7, Batch Number: 417, Loss: 2743.7288\n",
      "Time so far is 0m 6s\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "Epoch Number: 8, Batch Number: 48, Loss: 2429.3987\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 8, Batch Number: 148, Loss: 2338.5241\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 8, Batch Number: 248, Loss: 2453.2531\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 8, Batch Number: 348, Loss: 2221.4612\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 8, Batch Number: 448, Loss: 2126.1471\n",
      "Time so far is 0m 7s\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "Epoch Number: 9, Batch Number: 79, Loss: 2106.9746\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 9, Batch Number: 179, Loss: 1910.7587\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 9, Batch Number: 279, Loss: 2024.9927\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 9, Batch Number: 379, Loss: 2006.7640\n",
      "Time so far is 0m 7s\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "Epoch Number: 10, Batch Number: 10, Loss: 1859.2339\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 10, Batch Number: 110, Loss: 1851.2241\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 10, Batch Number: 210, Loss: 1708.5667\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 10, Batch Number: 310, Loss: 1865.7256\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 10, Batch Number: 410, Loss: 1842.7325\n",
      "Time so far is 0m 8s\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "Epoch Number: 11, Batch Number: 41, Loss: 1690.5130\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 11, Batch Number: 141, Loss: 1672.6343\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 11, Batch Number: 241, Loss: 1760.6565\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 11, Batch Number: 341, Loss: 1712.8752\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 11, Batch Number: 441, Loss: 1565.3908\n",
      "Time so far is 0m 9s\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "Epoch Number: 12, Batch Number: 72, Loss: 1576.2218\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 12, Batch Number: 172, Loss: 1612.2872\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 12, Batch Number: 272, Loss: 1541.0050\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 12, Batch Number: 372, Loss: 1595.1497\n",
      "Time so far is 0m 10s\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "Epoch Number: 13, Batch Number: 3, Loss: 1564.3615\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 13, Batch Number: 103, Loss: 1505.8679\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 13, Batch Number: 203, Loss: 1471.9064\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 13, Batch Number: 303, Loss: 1467.9715\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 13, Batch Number: 403, Loss: 1426.4925\n",
      "Time so far is 0m 11s\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "Epoch Number: 14, Batch Number: 34, Loss: 1562.0566\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 14, Batch Number: 134, Loss: 1313.2984\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 14, Batch Number: 234, Loss: 1268.0576\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 14, Batch Number: 334, Loss: 1251.9702\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 14, Batch Number: 434, Loss: 1314.0852\n",
      "Time so far is 0m 11s\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "Epoch Number: 15, Batch Number: 65, Loss: 1298.5234\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 15, Batch Number: 165, Loss: 1203.8896\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 15, Batch Number: 265, Loss: 1325.4428\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 15, Batch Number: 365, Loss: 1244.9741\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 15, Batch Number: 465, Loss: 1213.4383\n",
      "Time so far is 0m 12s\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "Epoch Number: 16, Batch Number: 96, Loss: 1341.5595\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 16, Batch Number: 196, Loss: 1233.8407\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 16, Batch Number: 296, Loss: 1217.4847\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 16, Batch Number: 396, Loss: 1164.7660\n",
      "Time so far is 0m 13s\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "Epoch Number: 17, Batch Number: 27, Loss: 1283.7909\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 17, Batch Number: 127, Loss: 1242.8629\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 17, Batch Number: 227, Loss: 1248.2983\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 17, Batch Number: 327, Loss: 1203.6178\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 17, Batch Number: 427, Loss: 1239.5565\n",
      "Time so far is 0m 14s\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "Epoch Number: 18, Batch Number: 58, Loss: 1187.5371\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 18, Batch Number: 158, Loss: 1186.5755\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 18, Batch Number: 258, Loss: 1277.6302\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 18, Batch Number: 358, Loss: 1264.3902\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 18, Batch Number: 458, Loss: 1137.2469\n",
      "Time so far is 0m 15s\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "Epoch Number: 19, Batch Number: 89, Loss: 1268.1766\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 19, Batch Number: 189, Loss: 1134.0427\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 19, Batch Number: 289, Loss: 1223.4829\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 19, Batch Number: 389, Loss: 1274.8022\n",
      "Time so far is 0m 15s\n",
      "\n",
      "Training complete in 0m 15s\n",
      "Best loss: 5693.797651\n"
     ]
    }
   ],
   "source": [
    "mnist_model1 = LogisticRegression(784, 10)\n",
    "mnist_model1, losses1 = train_mnist_model(mnist_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "LR is set to 0.001\n",
      "Epoch Number: 0, Batch Number: 100, Loss: 228.3035\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 0, Batch Number: 200, Loss: 220.8570\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 0, Batch Number: 300, Loss: 213.4622\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 0, Batch Number: 400, Loss: 204.9463\n",
      "Time so far is 0m 2s\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "Epoch Number: 1, Batch Number: 31, Loss: 194.8935\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 1, Batch Number: 131, Loss: 182.9734\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 1, Batch Number: 231, Loss: 171.3855\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 1, Batch Number: 331, Loss: 157.8285\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 1, Batch Number: 431, Loss: 144.7254\n",
      "Time so far is 0m 6s\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "Epoch Number: 2, Batch Number: 62, Loss: 132.8443\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 2, Batch Number: 162, Loss: 122.1539\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 2, Batch Number: 262, Loss: 111.6595\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 2, Batch Number: 362, Loss: 102.6710\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 2, Batch Number: 462, Loss: 94.8597\n",
      "Time so far is 0m 10s\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "Epoch Number: 3, Batch Number: 93, Loss: 88.1093\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 3, Batch Number: 193, Loss: 81.9604\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 3, Batch Number: 293, Loss: 76.1561\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 3, Batch Number: 393, Loss: 71.1153\n",
      "Time so far is 0m 13s\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "Epoch Number: 4, Batch Number: 24, Loss: 66.3666\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 4, Batch Number: 124, Loss: 62.6118\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 4, Batch Number: 224, Loss: 59.9168\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 4, Batch Number: 324, Loss: 57.0271\n",
      "Time so far is 0m 16s\n",
      "Epoch Number: 4, Batch Number: 424, Loss: 53.2630\n",
      "Time so far is 0m 17s\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "Epoch Number: 5, Batch Number: 55, Loss: 51.8571\n",
      "Time so far is 0m 17s\n",
      "Epoch Number: 5, Batch Number: 155, Loss: 49.3700\n",
      "Time so far is 0m 18s\n",
      "Epoch Number: 5, Batch Number: 255, Loss: 46.7385\n",
      "Time so far is 0m 19s\n",
      "Epoch Number: 5, Batch Number: 355, Loss: 46.0074\n",
      "Time so far is 0m 20s\n",
      "Epoch Number: 5, Batch Number: 455, Loss: 43.5577\n",
      "Time so far is 0m 21s\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "Epoch Number: 6, Batch Number: 86, Loss: 42.3016\n",
      "Time so far is 0m 21s\n",
      "Epoch Number: 6, Batch Number: 186, Loss: 40.5823\n",
      "Time so far is 0m 22s\n",
      "Epoch Number: 6, Batch Number: 286, Loss: 39.9328\n",
      "Time so far is 0m 23s\n",
      "Epoch Number: 6, Batch Number: 386, Loss: 40.1231\n",
      "Time so far is 0m 24s\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "LR is set to 0.0001\n",
      "Epoch Number: 7, Batch Number: 17, Loss: 37.9039\n",
      "Time so far is 0m 25s\n",
      "Epoch Number: 7, Batch Number: 117, Loss: 36.5139\n",
      "Time so far is 0m 25s\n",
      "Epoch Number: 7, Batch Number: 217, Loss: 36.7853\n",
      "Time so far is 0m 26s\n",
      "Epoch Number: 7, Batch Number: 317, Loss: 36.3333\n",
      "Time so far is 0m 27s\n",
      "Epoch Number: 7, Batch Number: 417, Loss: 36.4687\n",
      "Time so far is 0m 28s\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "Epoch Number: 8, Batch Number: 48, Loss: 36.5566\n",
      "Time so far is 0m 29s\n",
      "Epoch Number: 8, Batch Number: 148, Loss: 36.4505\n",
      "Time so far is 0m 29s\n",
      "Epoch Number: 8, Batch Number: 248, Loss: 35.2846\n",
      "Time so far is 0m 30s\n",
      "Epoch Number: 8, Batch Number: 348, Loss: 36.0274\n",
      "Time so far is 0m 31s\n",
      "Epoch Number: 8, Batch Number: 448, Loss: 35.3320\n",
      "Time so far is 0m 32s\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "Epoch Number: 9, Batch Number: 79, Loss: 35.3004\n",
      "Time so far is 0m 33s\n",
      "Epoch Number: 9, Batch Number: 179, Loss: 35.5394\n",
      "Time so far is 0m 34s\n",
      "Epoch Number: 9, Batch Number: 279, Loss: 34.7487\n",
      "Time so far is 0m 34s\n",
      "Epoch Number: 9, Batch Number: 379, Loss: 34.8340\n",
      "Time so far is 0m 35s\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "Epoch Number: 10, Batch Number: 10, Loss: 35.0751\n",
      "Time so far is 0m 36s\n",
      "Epoch Number: 10, Batch Number: 110, Loss: 34.5322\n",
      "Time so far is 0m 37s\n",
      "Epoch Number: 10, Batch Number: 210, Loss: 35.1636\n",
      "Time so far is 0m 38s\n",
      "Epoch Number: 10, Batch Number: 310, Loss: 34.1410\n",
      "Time so far is 0m 39s\n",
      "Epoch Number: 10, Batch Number: 410, Loss: 35.0931\n",
      "Time so far is 0m 40s\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "Epoch Number: 11, Batch Number: 41, Loss: 34.4250\n",
      "Time so far is 0m 41s\n",
      "Epoch Number: 11, Batch Number: 141, Loss: 34.3194\n",
      "Time so far is 0m 42s\n",
      "Epoch Number: 11, Batch Number: 241, Loss: 34.3187\n",
      "Time so far is 0m 42s\n",
      "Epoch Number: 11, Batch Number: 341, Loss: 34.1455\n",
      "Time so far is 0m 43s\n",
      "Epoch Number: 11, Batch Number: 441, Loss: 33.7787\n",
      "Time so far is 0m 44s\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "Epoch Number: 12, Batch Number: 72, Loss: 34.0056\n",
      "Time so far is 0m 45s\n",
      "Epoch Number: 12, Batch Number: 172, Loss: 34.4072\n",
      "Time so far is 0m 46s\n",
      "Epoch Number: 12, Batch Number: 272, Loss: 33.3367\n",
      "Time so far is 0m 47s\n",
      "Epoch Number: 12, Batch Number: 372, Loss: 33.9297\n",
      "Time so far is 0m 48s\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "Epoch Number: 13, Batch Number: 3, Loss: 32.6053\n",
      "Time so far is 0m 49s\n",
      "Epoch Number: 13, Batch Number: 103, Loss: 33.8025\n",
      "Time so far is 0m 50s\n",
      "Epoch Number: 13, Batch Number: 203, Loss: 32.9992\n",
      "Time so far is 0m 50s\n",
      "Epoch Number: 13, Batch Number: 303, Loss: 33.0505\n",
      "Time so far is 0m 51s\n",
      "Epoch Number: 13, Batch Number: 403, Loss: 33.1408\n",
      "Time so far is 0m 52s\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "Epoch Number: 14, Batch Number: 34, Loss: 33.5019\n",
      "Time so far is 0m 53s\n",
      "Epoch Number: 14, Batch Number: 134, Loss: 32.8437\n",
      "Time so far is 0m 54s\n",
      "Epoch Number: 14, Batch Number: 234, Loss: 32.1039\n",
      "Time so far is 0m 54s\n",
      "Epoch Number: 14, Batch Number: 334, Loss: 32.4018\n",
      "Time so far is 0m 55s\n",
      "Epoch Number: 14, Batch Number: 434, Loss: 33.3597\n",
      "Time so far is 0m 56s\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "Epoch Number: 15, Batch Number: 65, Loss: 32.7512\n",
      "Time so far is 0m 57s\n",
      "Epoch Number: 15, Batch Number: 165, Loss: 32.9764\n",
      "Time so far is 0m 58s\n",
      "Epoch Number: 15, Batch Number: 265, Loss: 33.5716\n",
      "Time so far is 0m 59s\n",
      "Epoch Number: 15, Batch Number: 365, Loss: 32.3160\n",
      "Time so far is 0m 60s\n",
      "Epoch Number: 15, Batch Number: 465, Loss: 32.6662\n",
      "Time so far is 1m 0s\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "Epoch Number: 16, Batch Number: 96, Loss: 32.6185\n",
      "Time so far is 1m 1s\n",
      "Epoch Number: 16, Batch Number: 196, Loss: 33.1703\n",
      "Time so far is 1m 2s\n",
      "Epoch Number: 16, Batch Number: 296, Loss: 31.8774\n",
      "Time so far is 1m 3s\n",
      "Epoch Number: 16, Batch Number: 396, Loss: 33.3712\n",
      "Time so far is 1m 4s\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "Epoch Number: 17, Batch Number: 27, Loss: 32.2711\n",
      "Time so far is 1m 5s\n",
      "Epoch Number: 17, Batch Number: 127, Loss: 32.7342\n",
      "Time so far is 1m 6s\n",
      "Epoch Number: 17, Batch Number: 227, Loss: 32.2960\n",
      "Time so far is 1m 7s\n",
      "Epoch Number: 17, Batch Number: 327, Loss: 33.1597\n",
      "Time so far is 1m 7s\n",
      "Epoch Number: 17, Batch Number: 427, Loss: 32.9428\n",
      "Time so far is 1m 8s\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "Epoch Number: 18, Batch Number: 58, Loss: 32.3273\n",
      "Time so far is 1m 9s\n",
      "Epoch Number: 18, Batch Number: 158, Loss: 33.2665\n",
      "Time so far is 1m 10s\n",
      "Epoch Number: 18, Batch Number: 258, Loss: 32.5412\n",
      "Time so far is 1m 11s\n",
      "Epoch Number: 18, Batch Number: 358, Loss: 32.8806\n",
      "Time so far is 1m 12s\n",
      "Epoch Number: 18, Batch Number: 458, Loss: 32.6500\n",
      "Time so far is 1m 12s\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "Epoch Number: 19, Batch Number: 89, Loss: 32.8752\n",
      "Time so far is 1m 13s\n",
      "Epoch Number: 19, Batch Number: 189, Loss: 32.2674\n",
      "Time so far is 1m 14s\n",
      "Epoch Number: 19, Batch Number: 289, Loss: 32.8333\n",
      "Time so far is 1m 15s\n",
      "Epoch Number: 19, Batch Number: 389, Loss: 32.1268\n",
      "Time so far is 1m 16s\n",
      "\n",
      "Training complete in 1m 16s\n",
      "Best loss: 153.023529\n"
     ]
    }
   ],
   "source": [
    "mnist_model2 = TwoLayerNN(784, 200, 200, 10)\n",
    "mnist_model2, losses2 = train_mnist_model(mnist_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"mnist_test.csv\", header = None).values\n",
    "test_labels, test_values = torch.LongTensor(test_data[:, 0]), torch.FloatTensor(test_data[:, 1:].astype(float))\n",
    "\n",
    "test_dataset = data.TensorDataset(test_values, test_labels)    \n",
    "loader_dset_test = data.DataLoader(test_dataset, batch_size=128, \n",
    "                                   num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Given a model that classifies data points and a labeled data set\n",
    "    this determines the accuracy of the model for a specific category. Here,\n",
    "    accuracy is defined as number of times the model guesses the right category\n",
    "    for the given category divided by the number of times that category appeared.\n",
    "    This is more precisely known as the precision.\n",
    "\"\"\"\n",
    "def accuracy_for_a_category(model, dataset, category):\n",
    "    positive_results = 0 # create counter for positive results\n",
    "    total = 0 # create counter for total \n",
    "    for datapoint, label in dataset: # creates loop to compute accuracy for all datapoints in dataset\n",
    "        if label == category:\n",
    "            total += 1 \n",
    "            _, guess = torch.max(model(Variable(datapoint)), 0)\n",
    "            positive_results += torch.sum(guess.data == torch.LongTensor([label]))\n",
    "    return positive_results/total\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "    Given a list of losses, this creates a plot of the loss curve.\n",
    "\"\"\"\n",
    "def plot_loss_curve(losses):\n",
    "    plt.plot(losses) # creates the matplot for losses over time\n",
    "\n",
    "\"\"\"\n",
    "    Given a model that classifies data points and a labeled data set this\n",
    "    determines the overall accuracy of the model.\n",
    "\"\"\"\n",
    "def test_model(model, dataset):\n",
    "    positive_results = 0\n",
    "    total = len(dataset)\n",
    "    for datapoint, label in dataset: \n",
    "        _, guess = torch.max(model(Variable(datapoint)), 0)\n",
    "        positive_results += torch.sum(guess.data == torch.LongTensor([label]))   \n",
    "    return positive_results/total\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the digit 0 is 0.98\n",
      "The accuracy on the digit 1 is 0.98\n",
      "The accuracy on the digit 2 is 0.90\n",
      "The accuracy on the digit 3 is 0.90\n",
      "The accuracy on the digit 4 is 0.93\n",
      "The accuracy on the digit 5 is 0.84\n",
      "The accuracy on the digit 6 is 0.94\n",
      "The accuracy on the digit 7 is 0.92\n",
      "The accuracy on the digit 8 is 0.87\n",
      "The accuracy on the digit 9 is 0.89\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRFKMgl9EJMUo+EVEUsz/BzwSKkiZStt1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17ac72198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy of the 2 layer neural net on MNIST is 0.92\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    accuracy_i = accuracy_for_a_category(mnist_model2, test_dataset, i)\n",
    "    print(\"The accuracy on the digit {} is {:.2f}\".format(i, accuracy_i))\n",
    "\n",
    "plot_loss_curve(losses2)\n",
    "plt.show()\n",
    "\n",
    "accuracy = test_model(mnist_model2, test_dataset)\n",
    "print(\"The overall accuracy of the 2 layer neural net on MNIST is {:.2f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
